# EnergySaver - The Application to help you track the production of green energy in your area

## Components

The application is idealized to work following a DataPipeline architecture, where the data is collected from a source, processed and stored in a database, and then used to generate insights and reports.

It will have two main components:

### Data Collector and Transformer

Responsible for collecting the data from the source, transforming it and storing it in a database.

This will follow the following architecture:

1. Data Collection

   The data will be collected from **public APIs**, which will be responsible for providing the data about the production of green energy/weather in a specific area.

   This data will be forwarded to the corresponding **kafka topics**, where it will be processed by the next step.

2. Data Transformation

   The data will be processed by a **Spark Streaming application**, which will be responsible for transforming the data and sending it into another **kafka topic**, where it will be consumed by the next step.

3. Data Storage

   A **MongoDB** database will be responsible for storing the data using its own _Kafka connector_. The data stored here will be used for the periodic training of the machine learning model.

### Data Analyzer and Reporter

Responsible for generating insights and reports based on the data stored in the database.

This will follow the following architecture:

1. Data Analysis

   The data will be read from the previous **kafka topic** and processed by a **Spark application**, there the previously trained Machine Learning model will be used to generate predictions about the data sending it into another **kafka topic**.

2. Data Reporting

   The same MongoDB database will read the data from the connector and store it.

   There will be a **FastAPI application** that will be providing both a REST API reading from the database and a **WebSocket** interface reading from the same kafka topic to access the data and the reports generated by the previous step.

## Technologies

The following technologies will be used in the project:

- **Kafka**: For the data streaming and processing
- **Spark**: For the data processing and machine learning model training
- **MongoDB**: For the data storage
- **FastAPI**: For the REST API and WebSocket interface
- **Docker**: For the containerization of the application
